{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d2b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pylab import mpl\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置matplotlib的配置\n",
    "# mpl.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体为黑体\n",
    "mpl.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b45455d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "USE_MULTI_GPU = True\n",
    "# 设置默认的CUDA设备\n",
    "torch.cuda.set_device(0)\n",
    "# 初始化CUDA环境\n",
    "torch.cuda.init()\n",
    "if USE_MULTI_GPU and torch.cuda.device_count() > 1:\n",
    "    MULTI_GPU = True\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"  # 设置所有六张显卡的编号\n",
    "    device_ids = ['0','1','2','3','4','5',] # 设置所有六张显卡的编号\n",
    "else:\n",
    "    MULTI_GPU = False\n",
    "    device_ids = ['0']\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(MULTI_GPU)\n",
    "deviceCount = torch.cuda.device_count()\n",
    "torch.cuda.set_device(device)\n",
    "print(deviceCount)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8712d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个字典，其中包含所有客户端的训练和测试指标列表\n",
    "loss_acc_r2_mse_mae_metrics = {\n",
    "    'client_0': {\n",
    "        'train': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        },\n",
    "        'test': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        }\n",
    "    },\n",
    "    'client_1': {\n",
    "        'train': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        },\n",
    "        'test': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        }\n",
    "    },\n",
    "    'client_2': {\n",
    "        'train': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        },\n",
    "        'test': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        }\n",
    "    },\n",
    "    'server_model': {\n",
    "        'test_volve': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        },\n",
    "        'test_xj': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        },\n",
    "        'test_bh': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "de_t_p_metrics = {\n",
    "    'client_0': {\n",
    "        'train': {'depth': [],'true': [],'pre': []},\n",
    "        'test': {'depth': [],'r2_size': [],'true': [],'pre': []}\n",
    "    },\n",
    "    'client_1': {\n",
    "        'train': {'depth': [], 'true': [], 'pre': []},\n",
    "        'test': {'depth': [], 'true': [], 'pre': []}\n",
    "    },\n",
    "    'client_2': {\n",
    "        'train': {'depth': [], 'true': [], 'pre': []},\n",
    "        'test': {'depth': [], 'true': [], 'pre': []}\n",
    "    },\n",
    "    'server_model': {\n",
    "        'test_volve': {'depth': [], 'true': [], 'pre': []},\n",
    "        'test_xj': {'depth': [], 'true': [], 'pre': []},\n",
    "        'test_bh': {'depth': [], 'true': [], 'pre': []},\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5c3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "volve_4 = './data/volve/volve_4.csv'\n",
    "volve_5 = './data/volve/volve_5.csv'\n",
    "volve_7 = './data/volve/volve_7.csv'\n",
    "volve_9 = './data/volve/volve_9.csv'\n",
    "volve_9A = './data/volve/volve_9A.csv'\n",
    "volve_10 = './data/volve/volve_10.csv'\n",
    "volve_12 = './data/volve/volve_12.csv'\n",
    "volve_14 = './data/volve/volve_14.csv'\n",
    "volve_15A = './data/volve/volve_15A.csv'\n",
    "volve_4_5_7_9A_10 = './data/volve/volve_4_5_7_9A_10.csv'\n",
    "volve_5_7_10_12 = './data/volve/volve_5_7_10_12.csv'\n",
    "xj_3 =  './data/xj/well_3.csv'\n",
    "xj_2 = './data/xj/well_2.csv'\n",
    "xj_1 = './data/xj/well_1.csv'\n",
    "\n",
    "bh_1 = './data/bh/bh_1.csv'\n",
    "bh_2 = './data/bh/bh_2.csv'\n",
    "bh_3 = './data/bh/bh_3.csv'\n",
    "bh_4 = './data/bh/bh_4.csv'\n",
    "bh_5 = './data/bh/bh_5.csv'\n",
    "bh_6 = './data/bh/bh_6.csv'\n",
    "bh_7 = './data/bh/bh_7.csv'\n",
    "bh_8 = './data/bh/bh_8.csv'\n",
    "bh_9 = './data/bh/bh_9.csv'\n",
    "bh_10 = './data/bh/bh_10.csv'\n",
    "bh_11 = './data/bh/bh_11.csv'\n",
    "bh_12 = './data/bh/bh_12.csv'\n",
    "bh_14 = './data/bh/bh_14.csv'\n",
    "bh_15 = './data/bh/bh_15.csv'\n",
    "bh_16 = './data/bh/bh_16csv'\n",
    "bh_7_15 = './data/bh/bh_7_15.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7afad5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_model_epoch = 100\n",
    "clint_model_epoch = 1\n",
    "\n",
    "model_pre_len = 50\n",
    "model_seq_len = 300\n",
    "model_tf_lr = 0.0002\n",
    "model_batch = 128\n",
    "model_feature_size=5\n",
    "model_d_model=512\n",
    "model_num_layers=1\n",
    "model_dropout=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713596b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # 64*512\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # 64*1\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # 256   model/2\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)  # 64*1*512\n",
    "\n",
    "    def forward(self, x):  # [seq,batch,d_model]\n",
    "        return x + self.pe[:x.size(0), :]  # 64*64*512\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=model_feature_size, d_model=model_d_model, num_layers=model_num_layers, dropout=model_dropout):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embedding = nn.Linear(feature_size, d_model)\n",
    "        self.dec_input_fc = nn.Linear(feature_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)  # 50*512\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=8, dropout=dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "        self.src_mask = None\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, src,tgt,tgt_mask):\n",
    "\n",
    "        # if self.src_key_padding_mask is None:\n",
    "        #     mask_key = src_padding  # [batch,seq]\n",
    "        #     self.src_key_padding_mask = mask_key\n",
    "        src_em = self.embedding(src)  # [seq,batch,d_model]\n",
    "        src_em_pos = self.pos_encoder(src_em)  # [seq,batch,d_model]\n",
    "        encoder_output = self.transformer_encoder(src_em_pos)\n",
    "\n",
    "        tgt_em = self.embedding(tgt)\n",
    "        tgt_em_pos = self.pos_encoder(tgt_em)\n",
    "\n",
    "        decoder_output = self.transformer_decoder(tgt_em_pos, encoder_output, tgt_mask=tgt_mask)\n",
    "\n",
    "        output = self.linear(decoder_output)\n",
    "        output_squeeze = output.squeeze()\n",
    "\n",
    "        self.tgt_mask = None\n",
    "        return output_squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce5d8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(TModel, loader,optimizer):\n",
    "    epoch_loss = 0\n",
    "    criterion = nn.MSELoss()  # 占位符 索引为0.9\n",
    "\n",
    "    for X, y in loader:\n",
    "        # X--[batch,seq,feature_size]  y--[batch,seq,feature_size]   64 300 13  64 50 13\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        mask = (torch.triu(torch.ones(y.size(1), y.size(1))) == 1).transpose(0, 1)\n",
    "        tgt_mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
    "\n",
    "        output = TModel(X, y, tgt_mask)\n",
    "        loss = criterion(output, y[:, :, -1])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(TModel.parameters(), 0.10)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "def test(TModel, tf_loader, y_max, y_min, de_max, de_min):\n",
    "    epoch_loss = 0\n",
    "    y_pre = []\n",
    "    y_true = []\n",
    "    y_depth = []\n",
    "    criterion = nn.MSELoss()  # 占位符 索引为0.9\n",
    "    for x, y in tf_loader:\n",
    "        with torch.no_grad():\n",
    "            label = y[:, :, -1].detach().view(1, len(y[:, :, -1]) * model_pre_len).squeeze()\n",
    "            label = label * (y_max - y_min) + y_min\n",
    "            label = label.numpy().tolist()\n",
    "            y_true += label\n",
    "\n",
    "            de = y[:, :, 0].detach().view(1, len(y[:, :, 0]) * model_pre_len).squeeze()\n",
    "            de = de * (de_max - de_min) + de_min\n",
    "            de = de.numpy().tolist()\n",
    "            y_depth += de\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            mask = (torch.triu(torch.ones(y.size(1), y.size(1))) == 1).transpose(0, 1)\n",
    "            tgt_mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
    "\n",
    "            output = TModel(x, y, tgt_mask)\n",
    "\n",
    "            loss = criterion(output, y[:, :, -1])\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            hat = output.cpu().detach().view(1, len(y[:, :, -1]) * model_pre_len).squeeze()\n",
    "            hat = hat * (y_max - y_min) + y_min\n",
    "            hat = hat.numpy().tolist()\n",
    "            y_pre += hat\n",
    "\n",
    "    label = np.array(y_true)\n",
    "    predict = np.array(y_pre)\n",
    "    dep = np.array(y_depth)\n",
    "\n",
    "    seq_label = label.reshape(int(len(label) / model_pre_len), model_pre_len)\n",
    "    seq_predict = predict.reshape(int(len(predict) / model_pre_len), model_pre_len)\n",
    "    seq_depth = dep.reshape(int(len(dep) / model_pre_len), model_pre_len)\n",
    "\n",
    "    true = np.concatenate((seq_label[:-1, 0], seq_label[-1, :]), axis=0)\n",
    "    depth = np.concatenate((seq_depth[:-1, 0], seq_depth[-1, :]), axis=0)\n",
    "    pre = averages(seq_predict)\n",
    "\n",
    "    r2 = r2_score(true, pre)\n",
    "    acc = 1 - (np.abs(pre - true) / (true + 1e-8)).mean()\n",
    "    mse = mean_squared_error(true, pre)\n",
    "\n",
    "    mae = mean_absolute_error(true, pre)\n",
    "\n",
    "    return acc, r2, mse, mae, epoch_loss, true, pre, depth\n",
    "def communication(server_model, models, client_weights):\n",
    "    with torch.no_grad():\n",
    "        for key in server_model.state_dict().keys():\n",
    "            if 'bn' not in key: # 跳过包含批量归一化（Batch Normalization，bn）的参数，因为批量归一化的参数在联邦学习中通常不需要像其他参数一样进行权重聚合\n",
    "                temp = torch.zeros_like(server_model.state_dict()[key], dtype=torch.float32) # 创建一个与当前参数相同形状和数据类型的零张量，用于存储加权聚合后的参数\n",
    "                for client_idx in range(client_num):\n",
    "                    temp += client_weights[client_idx] * models[client_idx].state_dict()[key] # 使用每个客户端的权重和对应模型的参数进行加权累加\n",
    "                server_model.state_dict()[key].data.copy_(temp) # 将聚合后的参数更新到服务器模型的对应位置\n",
    "                for client_idx in range(client_num):\n",
    "                    models[client_idx].state_dict()[key].data.copy_(server_model.state_dict()[key]) # 将聚合后的参数同步回每个客户端的模型\n",
    "    return server_model, models # 返回值：返回更新后的 server_model（服务器模型）和 models（客户端模型）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a1f4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(path):\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "   # data = data.iloc[::interval, :]\n",
    "\n",
    "    # data = data.clip(lower=0)  # 设置小于0的数都赋0\n",
    "    # data = data.apply(lambda x: x.mask((x < x.quantile(0.25) - 1.5 * (x.quantile(0.75) - x.quantile(0.25))) |\n",
    "    #                                      (x > x.quantile(0.75) + 1.5 * (\n",
    "    #                                                  x.quantile(0.75) - x.quantile(0.25)))).ffill().bfill())\n",
    "    #\n",
    "    # data = data.sort_values(by='MD')\n",
    "    # data=data.reset_index(drop=True)\n",
    "    data = data.astype('float32')\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.values\n",
    "\n",
    "    data_ =torch.tensor(data[:len(data)])\n",
    "    maxc, _ = data_.max(dim=0)\n",
    "    minc, _ = data_.min(dim=0)\n",
    "    y_max = maxc[-1]\n",
    "    y_min = minc[-1]\n",
    "    de_max = maxc[0]\n",
    "    de_min = minc[0]\n",
    "    data_ = (data_ - minc) / (maxc - minc)\n",
    "\n",
    "    data_last_index = data_.shape[0] - model_seq_len\n",
    "\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    for i in range(0, data_last_index - model_pre_len+1):\n",
    "        data_x = np.expand_dims(data_[i:i + model_seq_len], 0)  # [1,seq,feature_size]\n",
    "        data_y = np.expand_dims(data_[i + model_seq_len:i + model_seq_len + model_pre_len], 0)  # [1,seq,out_size]\n",
    "        data_X.append(data_x)\n",
    "        data_Y.append(data_y)\n",
    "\n",
    "    data_X=np.concatenate(data_X, axis=0)\n",
    "    data_Y = np.concatenate(data_Y, axis=0)\n",
    "\n",
    "    process_data = torch.from_numpy(data_X).type(torch.float32)\n",
    "    process_label = torch.from_numpy(data_Y).type(torch.float32)\n",
    "\n",
    "    data_feature_size = process_data.shape[-1]\n",
    "\n",
    "    dataset_train = TensorDataset(process_data, process_label)\n",
    "\n",
    "    data_dataloader = DataLoader(dataset_train, batch_size=model_batch, shuffle=False)\n",
    "    return data_dataloader,y_max,y_min, de_max,de_min\n",
    "\n",
    "def averages(matrix):  # 计算平均值\n",
    "    matrix = np.array(matrix)\n",
    "    row_count, col_count = matrix.shape\n",
    "    max_diagonal = row_count + col_count - 1\n",
    "    diagonals = np.zeros(max_diagonal)\n",
    "    counts = np.zeros(max_diagonal, dtype=int)\n",
    "    for i in range(row_count):\n",
    "        for j in range(col_count):\n",
    "            num = matrix[i, j]\n",
    "            diagonal_index = i + j\n",
    "            diagonals[diagonal_index] += num\n",
    "            counts[diagonal_index] += 1\n",
    "    averages = diagonals / counts\n",
    "    return averages\n",
    "\n",
    "def acc_loss_plot_one(train_data, type_, path):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_data, label='train_data', color='blue', linewidth=1)\n",
    "    plt.xlabel('epoch', fontsize=18)\n",
    "    plt.title(f'train_test_{type_}')\n",
    "    path_ = f'{path}'\n",
    "\n",
    "    plt.grid()\n",
    "    plt.savefig(path_)\n",
    "\n",
    "    plt.legend()\n",
    "   # plt.show()\n",
    "\n",
    "def acc_loss_plot_two(train_data, test_data, type_, path):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_data, label='train_data', color='blue', linewidth=1)\n",
    "    plt.plot(test_data, label='test_data', color='red', linewidth=1)\n",
    "    plt.xlabel('epoch', fontsize=18)\n",
    "    plt.title(f'train_test_{type_}')\n",
    "    path_ = f'{path}'\n",
    "\n",
    "    plt.grid()\n",
    "    plt.savefig(path_)\n",
    "\n",
    "    plt.legend()\n",
    "  #  plt.show()\n",
    "\n",
    "def true_test_plot(depth, true_data, predicted_data, type_, path):\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(depth, true_data, label='true_data', color='blue', linewidth=1)\n",
    "    plt.plot(depth, predicted_data, label='test_data', color='green', linewidth=1)\n",
    "    plt.ylabel(\"GRA\", fontsize=18)\n",
    "    plt.xlabel('depth', fontsize=18)\n",
    "    path_ = f'{path}'\n",
    "    plt.grid()\n",
    "    plt.savefig(path_)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68af345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f2f0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- server model epoch 0 ----------------------------------------\n",
      "     clint model epoch 0     \n",
      "volve\n",
      "  train:loss = 4.5829  acc = 0.3199  r2 = 0.7698 time =  2025-02-14 10:26:38\n",
      "  test:loss = 0.3174  acc = 0.8772  r2 = 0.8271 time =  2025-02-14 10:26:38\n",
      "xj\n",
      "  train:loss = 29.3121  acc = -5.8839  r2 = -3.8383 time =  2025-02-14 10:27:09\n",
      "  test:loss = 4.7856  acc = -0.7509  r2 = -5.2584 time =  2025-02-14 10:27:09\n",
      "bh\n",
      "  train:loss = 20.8725  acc = 0.3927  r2 = -0.0743 time =  2025-02-14 10:29:23\n",
      "  test:loss = 6.1581  acc = 0.3815  r2 = 0.0797 time =  2025-02-14 10:29:23\n",
      "     server model     \n",
      "\n",
      "volve test:loss = 0.5032  acc = 0.8711  r2 = 0.7593  mse = 27.7668  mae = 4.4549 time =  2025-02-14 10:29:28\n",
      "     server model     \n",
      "\n",
      "xj test:loss = 0.1503  acc = 0.7196  r2 = 0.8320  mse = 4.8418  mae = 1.8097 time =  2025-02-14 10:29:30\n",
      "     server model     \n",
      "\n",
      "bh test:loss = 2.6929  acc = 0.3118  r2 = 0.6265  mse = 9.1393  mae = 2.6373 time =  2025-02-14 10:29:41\n",
      "save clint model\n",
      "save server_model\n",
      "---------------------------------------- server model epoch 1 ----------------------------------------\n",
      "     clint model epoch 0     \n",
      "volve\n",
      "  train:loss = 0.9482  acc = 0.7160  r2 = 0.9543 time =  2025-02-14 10:33:07\n",
      "  test:loss = 0.2256  acc = 0.9254  r2 = 0.8782 time =  2025-02-14 10:33:07\n",
      "xj\n",
      "  train:loss = 1.1218  acc = -0.3665  r2 = 0.8161 time =  2025-02-14 10:33:38\n",
      "  test:loss = 0.0917  acc = 0.8336  r2 = 0.8832 time =  2025-02-14 10:33:38\n",
      "bh\n",
      "  train:loss = 8.8773  acc = 0.6135  r2 = 0.5445 time =  2025-02-14 10:35:53\n",
      "  test:loss = 2.6108  acc = 0.6084  r2 = 0.6112 time =  2025-02-14 10:35:53\n",
      "     server model     \n",
      "\n",
      "volve test:loss = 0.6747  acc = 0.8745  r2 = 0.6392  mse = 41.6271  mae = 5.4463 time =  2025-02-14 10:35:58\n",
      "     server model     \n",
      "\n",
      "xj test:loss = 0.1014  acc = 0.8827  r2 = 0.8743  mse = 3.6231  mae = 1.4263 time =  2025-02-14 10:35:59\n",
      "     server model     \n",
      "\n",
      "bh test:loss = 0.9690  acc = 0.7838  r2 = 0.8620  mse = 3.3781  mae = 1.3108 time =  2025-02-14 10:36:12\n",
      "save clint model\n",
      "save server_model\n",
      "---------------------------------------- server model epoch 2 ----------------------------------------\n",
      "     clint model epoch 0     \n",
      "volve\n",
      "  train:loss = 0.8159  acc = 0.7638  r2 = 0.9606 time =  2025-02-14 10:39:40\n",
      "  test:loss = 0.2095  acc = 0.9256  r2 = 0.8866 time =  2025-02-14 10:39:40\n",
      "xj\n",
      "  train:loss = 0.9125  acc = -0.3321  r2 = 0.8502 time =  2025-02-14 10:40:11\n",
      "  test:loss = 0.0515  acc = 0.8419  r2 = 0.9341 time =  2025-02-14 10:40:11\n",
      "bh\n",
      "  train:loss = 5.4641  acc = 0.7295  r2 = 0.7212 time =  2025-02-14 10:42:28\n",
      "  test:loss = 1.5597  acc = 0.7324  r2 = 0.7692 time =  2025-02-14 10:42:28\n",
      "     server model     \n",
      "\n",
      "volve test:loss = 0.3032  acc = 0.9139  r2 = 0.8369  mse = 18.8147  mae = 3.6519 time =  2025-02-14 10:42:33\n",
      "     server model     \n",
      "\n",
      "xj test:loss = 0.0443  acc = 0.9046  r2 = 0.9436  mse = 1.6261  mae = 0.9547 time =  2025-02-14 10:42:35\n",
      "     server model     \n",
      "\n",
      "bh test:loss = 0.4716  acc = 0.8148  r2 = 0.9317  mse = 1.6718  mae = 0.9555 time =  2025-02-14 10:42:46\n",
      "save clint model\n",
      "save server_model\n",
      "---------------------------------------- server model epoch 3 ----------------------------------------\n",
      "     clint model epoch 0     \n",
      "volve\n",
      "  train:loss = 1.0662  acc = 0.7516  r2 = 0.9481 time =  2025-02-14 10:46:15\n",
      "  test:loss = 0.2416  acc = 0.9142  r2 = 0.8685 time =  2025-02-14 10:46:15\n",
      "xj\n",
      "  train:loss = 0.4657  acc = 0.2241  r2 = 0.9248 time =  2025-02-14 10:46:46\n",
      "  test:loss = 0.0582  acc = 0.8639  r2 = 0.9252 time =  2025-02-14 10:46:46\n"
     ]
    }
   ],
   "source": [
    "volve_train,volve_train_y_max,volve_train_y_min, volve_train_de_max,volve_train_de_min = data_load(volve_5_7_10_12)\n",
    "volve_test,volve_test_y_max,volve_test_y_min, volve_test_de_max,volve_test_de_min =  data_load(volve_9A)\n",
    "\n",
    "xj_train,xj_train_y_max,xj_train_y_min, xj_train_de_max,xj_train_de_min =  data_load(xj_3)\n",
    "xj_test,xj_test_y_max,xj_test_y_min, xj_test_de_max,xj_test_de_min =  data_load(xj_1)\n",
    "\n",
    "bh_train,bh_train_y_max,bh_train_y_min, bh_train_de_max,bh_train_de_min =  data_load(bh_7_15)\n",
    "bh_test,bh_test_y_max,bh_test_y_min, bh_test_de_max,bh_test_de_min =  data_load(bh_10)\n",
    "\n",
    "train_loaders = [volve_train, xj_train, bh_train]\n",
    "test_loaders = [volve_test,xj_test,bh_test]\n",
    "\n",
    "train_y_maxs = [volve_train_y_max,xj_train_y_max,bh_train_y_max]\n",
    "train_y_mins = [volve_train_y_min,xj_train_y_min,bh_train_y_min]\n",
    "train_de_maxs = [volve_train_de_max,xj_train_de_max,bh_train_de_max]\n",
    "train_de_mins = [volve_train_de_min,xj_train_de_min,bh_train_de_min]\n",
    "\n",
    "test_y_max = [volve_test_y_max,xj_test_y_max,bh_test_y_max]\n",
    "test_y_min = [volve_test_y_min,xj_test_y_min,bh_test_y_min]\n",
    "test_de_max = [volve_test_de_max,xj_test_de_max,bh_test_de_max]\n",
    "test_de_min = [volve_test_de_min,xj_test_de_min,bh_test_de_min]\n",
    "\n",
    "datasets= ['volve', 'xj', 'bh']\n",
    "client_num = len(datasets) # 客户端的数量\n",
    "client_weights = [1 / client_num for i in range(client_num)]\n",
    "\n",
    "server_model=TransAm().to(device)\n",
    "models = [copy.deepcopy(server_model).to(device) for idx in range(client_num)]\n",
    "optimizers = [optim.Adam(params=models[idx].parameters(), lr=model_tf_lr, weight_decay=0.001) for idx in range(client_num)]\n",
    "\n",
    "for epoch in range(server_model_epoch):\n",
    "    print('-' * 40, 'server model epoch', epoch, '-' * 40)\n",
    "    for wi in range(clint_model_epoch):\n",
    "        print('    ', 'clint model epoch', wi, '    ')\n",
    "        for client_idx in range(client_num):\n",
    "            models[client_idx].train()\n",
    "            train(models[client_idx], train_loaders[client_idx],optimizers[client_idx])\n",
    "\n",
    "            models[client_idx].eval()\n",
    "            train_acc,train_r2, train_mse,train_mae,train_loss, true_train, pre_train, train_depth = test(models[client_idx],train_loaders[client_idx],\n",
    "                                                                                                           train_y_maxs[client_idx],train_y_mins[client_idx],\n",
    "                                                                                                           train_de_maxs[client_idx],train_de_mins[client_idx])\n",
    "\n",
    "            test_acc, test_r2, test_mse, test_mae, test_loss, true_test, pre_test, test_depth = test(models[client_idx],test_loaders[client_idx],\n",
    "                                                                                                     test_y_max[client_idx],test_y_min[client_idx],\n",
    "                                                                                                     test_de_max[client_idx],test_de_min[client_idx])\n",
    "            # 获取当前时间\n",
    "            now = datetime.now()\n",
    "            print(datasets[client_idx])\n",
    "            print('  train:loss =','{:.4f}'.format(train_loss), ' acc =', '{:.4f}'.format(train_acc), ' r2 =', '{:.4f}'.format(train_r2), 'time = ', now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "            print('  test:loss =','{:.4f}'.format(test_loss), ' acc =', '{:.4f}'.format(test_acc), ' r2 =', '{:.4f}'.format(test_r2), 'time = ', now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "            client_key = f'client_{client_idx}'  # 动态生成键名\n",
    "            loss_acc_r2_mse_mae_metrics[client_key]['train']['acc_size'].append(train_acc)\n",
    "            loss_acc_r2_mse_mae_metrics[client_key]['train']['r2_size'].append(train_r2)\n",
    "            loss_acc_r2_mse_mae_metrics[client_key]['train']['mse_size'].append(train_mse)\n",
    "            loss_acc_r2_mse_mae_metrics[client_key]['train']['mae_size'].append(train_mae)\n",
    "            loss_acc_r2_mse_mae_metrics[client_key]['train']['loss_size'].append(train_loss)\n",
    "            loss_acc_r2_mse_mae_metrics[client_key]['test']['acc_size'].append(test_acc)\n",
    "            loss_acc_r2_mse_mae_metrics[client_key]['test']['r2_size'].append(test_r2)\n",
    "            loss_acc_r2_mse_mae_metrics[client_key]['test']['mse_size'].append(test_mse)\n",
    "            loss_acc_r2_mse_mae_metrics[client_key]['test']['mae_size'].append(test_mae)\n",
    "            loss_acc_r2_mse_mae_metrics[client_key]['test']['loss_size'].append(test_loss)\n",
    "\n",
    "            de_t_p_metrics[client_key]['train']['depth'] = train_depth\n",
    "            de_t_p_metrics[client_key]['train']['true'] = true_train\n",
    "            de_t_p_metrics[client_key]['train']['pre'] = pre_train\n",
    "            de_t_p_metrics[client_key]['test']['depth'] = test_depth\n",
    "            de_t_p_metrics[client_key]['test']['true'] = true_test\n",
    "            de_t_p_metrics[client_key]['test']['pre'] = pre_test\n",
    "\n",
    "    server_model, models = communication(server_model, models, client_weights)\n",
    "\n",
    "    for client_idx in range(client_num):\n",
    "        server_model.eval()\n",
    "        test_acc, test_r2, test_mse, test_mae, test_loss, true_test, pre_test, test_depth = test(server_model,test_loaders[client_idx],\n",
    "                                                                                                 test_y_max[client_idx],test_y_min[client_idx],\n",
    "                                                                                                 test_de_max[client_idx],test_de_min[client_idx])\n",
    "        client_key = f'test_{datasets[client_idx]}'  # 动态生成键名\n",
    "        loss_acc_r2_mse_mae_metrics['server_model'][client_key]['acc_size'].append(test_acc)\n",
    "        loss_acc_r2_mse_mae_metrics['server_model'][client_key]['r2_size'].append(test_r2)\n",
    "        loss_acc_r2_mse_mae_metrics['server_model'][client_key]['mse_size'].append(test_mse)\n",
    "        loss_acc_r2_mse_mae_metrics['server_model'][client_key]['mae_size'].append(test_mae)\n",
    "        loss_acc_r2_mse_mae_metrics['server_model'][client_key]['loss_size'].append(test_loss)\n",
    "\n",
    "        de_t_p_metrics['server_model'][client_key]['depth'] = test_depth\n",
    "        de_t_p_metrics['server_model'][client_key]['true'] = true_test\n",
    "        de_t_p_metrics['server_model'][client_key]['pre'] = pre_test\n",
    "\n",
    "        # 获取当前时间\n",
    "        now = datetime.now()\n",
    "\n",
    "        print('    ', 'server model', '    ')\n",
    "        print()\n",
    "        print(datasets[client_idx],'test:loss =', '{:.4f}'.format(test_loss), ' acc =', '{:.4f}'.format(test_acc), ' r2 =','{:.4f}'.format(test_r2),\n",
    "              ' mse =', '{:.4f}'.format(test_mse), ' mae =', '{:.4f}'.format(test_mae),'time = ', now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    server_volve_loss_acc_mse_mae_dict = {'test_loss': loss_acc_r2_mse_mae_metrics['server_model']['test_volve']['loss_size'],\n",
    "                                    'test_acc': loss_acc_r2_mse_mae_metrics['server_model']['test_volve']['acc_size'],\n",
    "                                    'test_r2': loss_acc_r2_mse_mae_metrics['server_model']['test_volve']['r2_size'],\n",
    "                                    'test_mse': loss_acc_r2_mse_mae_metrics['server_model']['test_volve']['mse_size'],\n",
    "                                    'test_mae': loss_acc_r2_mse_mae_metrics['server_model']['test_volve']['mae_size'] }\n",
    "    server_volve_pre_ture_test_dict = {'depth': de_t_p_metrics['server_model']['test_volve']['depth'],\n",
    "                                 'true': de_t_p_metrics['server_model']['test_volve']['true'],\n",
    "                                 'pre': de_t_p_metrics['server_model']['test_volve']['pre']}\n",
    "    server_xj_loss_acc_mse_mae_dict = {'test_loss': loss_acc_r2_mse_mae_metrics['server_model']['test_xj']['loss_size'],\n",
    "                                    'test_acc': loss_acc_r2_mse_mae_metrics['server_model']['test_xj']['acc_size'],\n",
    "                                    'test_r2': loss_acc_r2_mse_mae_metrics['server_model']['test_xj']['r2_size'],\n",
    "                                    'test_mse': loss_acc_r2_mse_mae_metrics['server_model']['test_xj']['mse_size'],\n",
    "                                    'test_mae': loss_acc_r2_mse_mae_metrics['server_model']['test_xj']['mae_size'] }\n",
    "    server_xj_pre_ture_test_dict = {'depth': de_t_p_metrics['server_model']['test_xj']['depth'],\n",
    "                                       'true': de_t_p_metrics['server_model']['test_xj']['true'],\n",
    "                                       'pre': de_t_p_metrics['server_model']['test_xj']['pre']}\n",
    "    server_bh_loss_acc_mse_mae_dict = {'test_loss': loss_acc_r2_mse_mae_metrics['server_model']['test_bh']['loss_size'],\n",
    "                                    'test_acc': loss_acc_r2_mse_mae_metrics['server_model']['test_bh']['acc_size'],\n",
    "                                    'test_r2': loss_acc_r2_mse_mae_metrics['server_model']['test_bh']['r2_size'],\n",
    "                                    'test_mse': loss_acc_r2_mse_mae_metrics['server_model']['test_bh']['mse_size'],\n",
    "                                    'test_mae': loss_acc_r2_mse_mae_metrics['server_model']['test_bh']['mae_size'] }\n",
    "    server_bh_pre_ture_test_dict = {'depth': de_t_p_metrics['server_model']['test_bh']['depth'],\n",
    "                                       'true': de_t_p_metrics['server_model']['test_bh']['true'],\n",
    "                                       'pre': de_t_p_metrics['server_model']['test_bh']['pre']}\n",
    "\n",
    "    clint0_volve__loss_acc_mse_mae_dict = {'test_loss': loss_acc_r2_mse_mae_metrics['client_0']['test']['loss_size'],\n",
    "                                'test_acc': loss_acc_r2_mse_mae_metrics['client_0']['test']['acc_size'],\n",
    "                                'test_r2': loss_acc_r2_mse_mae_metrics['client_0']['test']['r2_size'],\n",
    "                                'test_mse': loss_acc_r2_mse_mae_metrics['client_0']['test']['mse_size'],\n",
    "                                'test_mae': loss_acc_r2_mse_mae_metrics['client_0']['test']['mae_size'],\n",
    "                                'train_loss': loss_acc_r2_mse_mae_metrics['client_0']['train']['loss_size'],\n",
    "                                'train_acc': loss_acc_r2_mse_mae_metrics['client_0']['train']['acc_size'],\n",
    "                                'train_r2': loss_acc_r2_mse_mae_metrics['client_0']['train']['r2_size'],\n",
    "                                'train_mse': loss_acc_r2_mse_mae_metrics['client_0']['train']['mse_size'],\n",
    "                                'train_mae': loss_acc_r2_mse_mae_metrics['client_0']['train']['mae_size'],\n",
    "                                }\n",
    "    clint0_volve_pre_ture_test_dict = {'depth': de_t_p_metrics['client_0']['test']['depth'],\n",
    "                                    'true': de_t_p_metrics['client_0']['test']['true'],\n",
    "                                    'pre': de_t_p_metrics['client_0']['test']['pre'] }\n",
    "    clint1_xj_loss_acc_mse_mae_dict = {'test_loss': loss_acc_r2_mse_mae_metrics['client_1']['test']['loss_size'],\n",
    "                                'test_acc': loss_acc_r2_mse_mae_metrics['client_1']['test']['acc_size'],\n",
    "                                'test_r2': loss_acc_r2_mse_mae_metrics['client_1']['test']['r2_size'],\n",
    "                                'test_mse': loss_acc_r2_mse_mae_metrics['client_1']['test']['mse_size'],\n",
    "                                'test_mae': loss_acc_r2_mse_mae_metrics['client_1']['test']['mae_size'],\n",
    "                                'train_loss': loss_acc_r2_mse_mae_metrics['client_1']['train']['loss_size'],\n",
    "                                'train_acc': loss_acc_r2_mse_mae_metrics['client_1']['train']['acc_size'],\n",
    "                                'train_r2': loss_acc_r2_mse_mae_metrics['client_1']['train']['r2_size'],\n",
    "                                'train_mse': loss_acc_r2_mse_mae_metrics['client_1']['train']['mse_size'],\n",
    "                                'train_mae': loss_acc_r2_mse_mae_metrics['client_1']['train']['mae_size'],\n",
    "                                }\n",
    "    clint1_xj_pre_ture_test_dict = {'depth': de_t_p_metrics['client_1']['test']['depth'],\n",
    "                                    'true': de_t_p_metrics['client_1']['test']['true'],\n",
    "                                    'pre': de_t_p_metrics['client_1']['test']['pre'] }\n",
    "    clint2_bh_loss_acc_mse_mae_dict = {'test_loss': loss_acc_r2_mse_mae_metrics['client_2']['test']['loss_size'],\n",
    "                                'test_acc': loss_acc_r2_mse_mae_metrics['client_2']['test']['acc_size'],\n",
    "                                'test_r2': loss_acc_r2_mse_mae_metrics['client_2']['test']['r2_size'],\n",
    "                                'test_mse': loss_acc_r2_mse_mae_metrics['client_2']['test']['mse_size'],\n",
    "                                'test_mae': loss_acc_r2_mse_mae_metrics['client_2']['test']['mae_size'],\n",
    "                                'train_loss': loss_acc_r2_mse_mae_metrics['client_2']['train']['loss_size'],\n",
    "                                'train_acc': loss_acc_r2_mse_mae_metrics['client_2']['train']['acc_size'],\n",
    "                                'train_r2': loss_acc_r2_mse_mae_metrics['client_2']['train']['r2_size'],\n",
    "                                'train_mse': loss_acc_r2_mse_mae_metrics['client_2']['train']['mse_size'],\n",
    "                                'train_mae': loss_acc_r2_mse_mae_metrics['client_2']['train']['mae_size'],\n",
    "                                }\n",
    "    clint2_bh_pre_ture_test_dict = {'depth': de_t_p_metrics['client_2']['test']['depth'],\n",
    "                                    'true': de_t_p_metrics['client_2']['test']['true'],\n",
    "                                    'pre': de_t_p_metrics['client_2']['test']['pre'] }\n",
    "\n",
    "    server_volve_loss_acc_mse_mae = pd.DataFrame(server_volve_loss_acc_mse_mae_dict)\n",
    "    server_volve_pre_ture_test = pd.DataFrame(server_volve_pre_ture_test_dict)\n",
    "    server_xj_loss_acc_mse_mae = pd.DataFrame(server_xj_loss_acc_mse_mae_dict)\n",
    "    server_xj_pre_ture_test = pd.DataFrame(server_xj_pre_ture_test_dict)\n",
    "    server_bh_loss_acc_mse_mae = pd.DataFrame(server_bh_loss_acc_mse_mae_dict)\n",
    "    server_bh_pre_ture_test = pd.DataFrame(server_bh_pre_ture_test_dict)\n",
    "\n",
    "    clint0_volve_loss_acc_mse_mae = pd.DataFrame(clint0_volve__loss_acc_mse_mae_dict)\n",
    "    clint0_volve_pre_ture_test = pd.DataFrame(clint0_volve_pre_ture_test_dict)\n",
    "    clint1_xj_loss_acc_mse_mae = pd.DataFrame(clint1_xj_loss_acc_mse_mae_dict)\n",
    "    clint1_xj_pre_ture_test = pd.DataFrame(clint1_xj_pre_ture_test_dict)\n",
    "    clint2_bh_loss_acc_mse_mae = pd.DataFrame(clint2_bh_loss_acc_mse_mae_dict)\n",
    "    clint2_bh_pre_ture_test = pd.DataFrame(clint2_bh_pre_ture_test_dict)\n",
    "\n",
    "    server_volve_loss_acc_mse_mae.to_csv('./out1/server/volve/server_volve_loss_acc_mse_mae.csv', sep=\",\", index=True)\n",
    "    server_volve_pre_ture_test.to_csv('./out1/server/volve/server_volve_pre_ture_test.csv', sep=\",\", index=True)\n",
    "    server_xj_loss_acc_mse_mae.to_csv('./out1/server/xj/server_xj_loss_acc_mse_mae.csv', sep=\",\", index=True)\n",
    "    server_xj_pre_ture_test.to_csv('./out1/server/xj/server_xj_pre_ture_test.csv', sep=\",\", index=True)\n",
    "    server_bh_loss_acc_mse_mae.to_csv('./out1/server/bh/server_bh_loss_acc_mse_mae.csv', sep=\",\", index=True)\n",
    "    server_bh_pre_ture_test.to_csv('./out1/server/bh/server_bh_pre_ture_test.csv', sep=\",\", index=True)\n",
    "\n",
    "    clint0_volve_loss_acc_mse_mae.to_csv('./out1/client/volve/clint0_volve_loss_acc_mse_mae.csv', sep=\",\", index=True)\n",
    "    clint0_volve_pre_ture_test.to_csv('./out1/client/volve/clint0_volve_pre_ture_test.csv', sep=\",\", index=True)\n",
    "    clint1_xj_loss_acc_mse_mae.to_csv('./out1/client/xj/clint1_xj_loss_acc_mse_mae.csv', sep=\",\", index=True)\n",
    "    clint1_xj_pre_ture_test.to_csv('./out1/client/xj/clint1_xj_pre_ture_test.csv', sep=\",\", index=True)\n",
    "    clint2_bh_loss_acc_mse_mae.to_csv('./out1/client/bh/clint2_bh_loss_acc_mse_mae.csv', sep=\",\", index=True)\n",
    "    clint2_bh_pre_ture_test.to_csv('./out1/client/bh/clint2_bh_pre_ture_test.csv', sep=\",\", index=True)\n",
    "\n",
    "    acc_loss_plot_one(server_volve_loss_acc_mse_mae['test_loss'], 'loss','./out1/server/volve/server_test_loss.png')\n",
    "    acc_loss_plot_one(server_volve_loss_acc_mse_mae['test_r2'], 'r2', './out1/server/volve/server_test_r2.png')\n",
    "    acc_loss_plot_one(server_xj_loss_acc_mse_mae['test_loss'], 'loss','./out1/server/xj/server_test_loss.png')\n",
    "    acc_loss_plot_one(server_xj_loss_acc_mse_mae['test_r2'], 'r2', './out1/server/xj/server_test_r2.png')\n",
    "    acc_loss_plot_one(server_bh_loss_acc_mse_mae['test_loss'], 'loss','./out1/server/bh/server_test_loss.png')\n",
    "    acc_loss_plot_one(server_bh_loss_acc_mse_mae['test_r2'], 'r2', './out1/server/bh/server_test_r2.png')\n",
    "\n",
    "    acc_loss_plot_two(clint0_volve_loss_acc_mse_mae['train_r2'], clint0_volve_loss_acc_mse_mae['test_r2'], 'r2','./out1/client/volve/clint0_volve_r2.png')\n",
    "    acc_loss_plot_two(clint0_volve_loss_acc_mse_mae['train_loss'], clint0_volve_loss_acc_mse_mae['test_loss'], 'r2','./out1/client/volve/clint0_volve_loss.png')\n",
    "    acc_loss_plot_two(clint1_xj_loss_acc_mse_mae['train_r2'], clint1_xj_loss_acc_mse_mae['test_r2'], 'r2','./out1/client/xj/clint1_xj_r2.png')\n",
    "    acc_loss_plot_two(clint1_xj_loss_acc_mse_mae['train_loss'], clint1_xj_loss_acc_mse_mae['test_loss'], 'r2','./out1/client/xj/clint1_xj_loss.png')\n",
    "    acc_loss_plot_two(clint2_bh_loss_acc_mse_mae['train_r2'], clint2_bh_loss_acc_mse_mae['test_r2'], 'r2','./out1/client/bh/clint2_bh_r2.png')\n",
    "    acc_loss_plot_two(clint2_bh_loss_acc_mse_mae['train_loss'], clint2_bh_loss_acc_mse_mae['test_loss'], 'r2','./out1/client/bh/clint2_bh_loss.png')\n",
    "\n",
    "    true_test_plot(server_volve_pre_ture_test['depth'], server_volve_pre_ture_test['true'], server_volve_pre_ture_test['pre'], 'test',\n",
    "                   './out1/server/volve/server_volve_pre_ture_test.png')\n",
    "    true_test_plot(server_xj_pre_ture_test['depth'], server_xj_pre_ture_test['true'], server_xj_pre_ture_test['pre'], 'test',\n",
    "                   './out1/server/xj/server_xj_pre_ture_test.png')\n",
    "    true_test_plot(server_bh_pre_ture_test['depth'], server_bh_pre_ture_test['true'], server_bh_pre_ture_test['pre'], 'test',\n",
    "                   './out1/server/bh/server_bh_pre_ture_test.png')\n",
    "    true_test_plot(clint0_volve_pre_ture_test['depth'], clint0_volve_pre_ture_test['true'], clint0_volve_pre_ture_test['pre'], 'test',\n",
    "                   './out1/client/volve/clint0_volve_pre_ture_test.png')\n",
    "    true_test_plot(clint1_xj_pre_ture_test['depth'], clint1_xj_pre_ture_test['true'], clint1_xj_pre_ture_test['pre'], 'test',\n",
    "                   './out1/client/xj/clint1_xj_pre_ture_test.png')\n",
    "    true_test_plot(clint2_bh_pre_ture_test['depth'], clint2_bh_pre_ture_test['true'], clint2_bh_pre_ture_test['pre'], 'test',\n",
    "                   './out1/client/bh/clint2_bh_pre_ture_test.png')\n",
    "\n",
    "    torch.save(models[0].state_dict(), './out1/model/model_0_volve.pkl')\n",
    "    torch.save(models[1].state_dict(), './out1/model/model_1_xj.pkl')\n",
    "    torch.save(models[2].state_dict(), './out1/model/model_2_bh.pkl')\n",
    "    print('save clint model')\n",
    "    torch.save(server_model.state_dict(), './out1/model/server_model.pkl')\n",
    "    print('save server_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c2de0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02984b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6282c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198796c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ly3610",
   "language": "python",
   "name": "py3610"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
