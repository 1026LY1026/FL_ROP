{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe791b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model_pre_len = 300\n",
    "model_seq_len = 300\n",
    "model_tf_lr = 0.00015\n",
    "model_batch = 128\n",
    "model_feature_size=5\n",
    "model_d_model=512\n",
    "model_num_layers=1\n",
    "model_dropout=0\n",
    "\n",
    "\n",
    "USE_MULTI_GPU = True\n",
    "# 设置默认的CUDA设备\n",
    "torch.cuda.set_device(0)\n",
    "# 初始化CUDA环境\n",
    "torch.cuda.init()\n",
    "if USE_MULTI_GPU and torch.cuda.device_count() > 1:\n",
    "    MULTI_GPU = True\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"  # 设置所有六张显卡的编号\n",
    "    device_ids = ['0','1','2','3','4','5',] # 设置所有六张显卡的编号\n",
    "else:\n",
    "    MULTI_GPU = False\n",
    "    device_ids = ['0']\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(MULTI_GPU)\n",
    "deviceCount = torch.cuda.device_count()\n",
    "torch.cuda.set_device(device)\n",
    "print(deviceCount)\n",
    "print(device)\n",
    "\n",
    "\n",
    "volve_4 = './data/volve/volve_4.csv'\n",
    "volve_5 = './data/volve/volve_5.csv'\n",
    "volve_7 = './data/volve/volve_7.csv'\n",
    "volve_9 = './data/volve/volve_9.csv'\n",
    "volve_9A = './data/volve/volve_9A.csv'\n",
    "volve_10 = './data/volve/volve_10.csv'\n",
    "volve_12 = './data/volve/volve_12.csv'\n",
    "volve_14 = './data/volve/volve_14.csv'\n",
    "volve_15A = './data/volve/volve_15A.csv'\n",
    "volve_4_5_7_9A_10 = './data/volve/volve_4_5_7_9A_10.csv'\n",
    "xj_3 =  './data/xj/well_3.csv'\n",
    "xj_2 = './data/xj/well_2.csv'\n",
    "xj_1 = './data/xj/well_1.csv'\n",
    "\n",
    "bh_1 = './data/bh/bh_1.csv'\n",
    "bh_2 = './data/bh/bh_2.csv'\n",
    "bh_3 = './data/bh/bh_3.csv'\n",
    "bh_4 = './data/bh/bh_4.csv'\n",
    "bh_5 = './data/bh/bh_5.csv'\n",
    "bh_6 = './data/bh/bh_6.csv'\n",
    "bh_7 = './data/bh/bh_7.csv'\n",
    "bh_8 = './data/bh/bh_8.csv'\n",
    "bh_9 = './data/bh/bh_9.csv'\n",
    "bh_10 = './data/bh/bh_10.csv'\n",
    "bh_11 = './data/bh/bh_11.csv'\n",
    "bh_12 = './data/bh/bh_12.csv'\n",
    "bh_14 = './data/bh/bh_14.csv'\n",
    "bh_15 = './data/bh/bh_15.csv'\n",
    "bh_16 = './data/bh/bh_16.csv'\n",
    "bh_7_15 = './data/bh/bh_7_15.csv'\n",
    "# 创建一个字典，其中包含所有客户端的训练和测试指标列表\n",
    "loss_acc_r2_mse_mae_metrics = {\n",
    "    'client_0': {\n",
    "        'train': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        },\n",
    "        'test': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        }\n",
    "    },\n",
    "    'client_1': {\n",
    "        'train': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        },\n",
    "        'test': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        }\n",
    "    },\n",
    "    'client_2': {\n",
    "        'train': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        },\n",
    "        'test': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        }\n",
    "    },\n",
    "    'server_model': {\n",
    "        'test_volve': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        },\n",
    "        'test_xj': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        },\n",
    "        'test_bh': {\n",
    "            'acc_size': [],'r2_size': [],'mse_size': [],'mae_size': [],'loss_size': []\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "de_t_p_metrics = {\n",
    "    'client_0': {\n",
    "        'train': {'depth': [],'true': [],'pre': []},\n",
    "        'test': {'depth': [],'r2_size': [],'true': [],'pre': []}\n",
    "    },\n",
    "    'client_1': {\n",
    "        'train': {'depth': [], 'true': [], 'pre': []},\n",
    "        'test': {'depth': [], 'true': [], 'pre': []}\n",
    "    },\n",
    "    'client_2': {\n",
    "        'train': {'depth': [], 'true': [], 'pre': []},\n",
    "        'test': {'depth': [], 'true': [], 'pre': []}\n",
    "    },\n",
    "    'server_model': {\n",
    "        'test_volve': {'depth': [], 'true': [], 'pre': []},\n",
    "        'test_xj': {'depth': [], 'true': [], 'pre': []},\n",
    "        'test_bh': {'depth': [], 'true': [], 'pre': []},\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f8ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pylab import mpl\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置matplotlib的配置\n",
    "# mpl.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体为黑体\n",
    "mpl.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # 64*512\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # 64*1\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # 256   model/2\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)  # 64*1*512\n",
    "\n",
    "    def forward(self, x):  # [seq,batch,d_model]\n",
    "        return x + self.pe[:x.size(0), :]  # 64*64*512\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=model_feature_size, d_model=model_d_model, num_layers=model_num_layers, dropout=model_dropout):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embedding = nn.Linear(feature_size, d_model)\n",
    "        self.dec_input_fc = nn.Linear(feature_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)  # 50*512\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=8, dropout=dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "        self.src_mask = None\n",
    "        self.src_key_padding_mask = None\n",
    "\n",
    "    def forward(self, src,tgt,tgt_mask):\n",
    "\n",
    "        # if self.src_key_padding_mask is None:\n",
    "        #     mask_key = src_padding  # [batch,seq]\n",
    "        #     self.src_key_padding_mask = mask_key\n",
    "        src_em = self.embedding(src)  # [seq,batch,d_model]\n",
    "        src_em_pos = self.pos_encoder(src_em)  # [seq,batch,d_model]\n",
    "        encoder_output = self.transformer_encoder(src_em_pos)\n",
    "\n",
    "        tgt_em = self.embedding(tgt)\n",
    "        tgt_em_pos = self.pos_encoder(tgt_em)\n",
    "\n",
    "        decoder_output = self.transformer_decoder(tgt_em_pos, encoder_output, tgt_mask=tgt_mask)\n",
    "\n",
    "        output = self.linear(decoder_output)\n",
    "        output_squeeze = output.squeeze()\n",
    "\n",
    "        self.tgt_mask = None\n",
    "        return output_squeeze\n",
    "\n",
    "\n",
    "def test(TModel, tf_loader, y_max, y_min, de_max, de_min):\n",
    "    y_pre = []\n",
    "    y_true = []\n",
    "    y_depth = []\n",
    "\n",
    "    for x, y in tf_loader:\n",
    "        with torch.no_grad():\n",
    "            label = y[:, :, -1].detach().view(1, len(y[:, :, -1]) * model_pre_len).squeeze()\n",
    "            label = label * (y_max - y_min) + y_min\n",
    "            label = label.numpy().tolist()\n",
    "            y_true += label\n",
    "\n",
    "            de = y[:, :, 0].detach().view(1, len(y[:, :, 0]) * model_pre_len).squeeze()\n",
    "            de = de * (de_max - de_min) + de_min\n",
    "            de = de.numpy().tolist()\n",
    "            y_depth += de\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            mask = (torch.triu(torch.ones(y.size(1), y.size(1))) == 1).transpose(0, 1)\n",
    "            tgt_mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
    "\n",
    "            output = TModel(x, y, tgt_mask)\n",
    "\n",
    "            hat = output.cpu().detach().view(1, len(y[:, :, -1]) * model_pre_len).squeeze()\n",
    "            hat = hat * (y_max - y_min) + y_min\n",
    "            hat = hat.numpy().tolist()\n",
    "            y_pre += hat\n",
    "\n",
    "    label = np.array(y_true)\n",
    "    predict = np.array(y_pre)\n",
    "    dep = np.array(y_depth)\n",
    "\n",
    "    seq_label = label.reshape(int(len(label) / model_pre_len), model_pre_len)\n",
    "    seq_predict = predict.reshape(int(len(predict) / model_pre_len), model_pre_len)\n",
    "    seq_depth = dep.reshape(int(len(dep) / model_pre_len), model_pre_len)\n",
    "\n",
    "    true = np.concatenate((seq_label[:-1, 0], seq_label[-1, :]), axis=0)\n",
    "    depth = np.concatenate((seq_depth[:-1, 0], seq_depth[-1, :]), axis=0)\n",
    "    pre = averages(seq_predict)\n",
    "\n",
    "    r2 = r2_score(true, pre)\n",
    "    acc = 1 - (np.abs(pre - true) / (true + 1e-8)).mean()\n",
    "    mse = mean_squared_error(true, pre)\n",
    "\n",
    "    mae = mean_absolute_error(true, pre)\n",
    "\n",
    "    return acc, r2, mse, mae, true, pre, depth\n",
    "\n",
    "def data_load(path):\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "    # data = data[['MD', 'TVD', 'RPMA', 'WOBA', 'ROPA']]\n",
    "   # data = data.iloc[::interval, :]\n",
    "\n",
    "    # data = data.clip(lower=0)  # 设置小于0的数都赋0\n",
    "    # data = data.apply(lambda x: x.mask((x < x.quantile(0.25) - 1.5 * (x.quantile(0.75) - x.quantile(0.25))) |\n",
    "    #                                      (x > x.quantile(0.75) + 1.5 * (\n",
    "    #                                                  x.quantile(0.75) - x.quantile(0.25)))).ffill().bfill())\n",
    "    #\n",
    "    # data = data.sort_values(by='MD')\n",
    "    # data=data.reset_index(drop=True)\n",
    "    data = data.astype('float32')\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.values\n",
    "\n",
    "    data_ =torch.tensor(data[:len(data)])\n",
    "    maxc, _ = data_.max(dim=0)\n",
    "    minc, _ = data_.min(dim=0)\n",
    "    y_max = maxc[-1]\n",
    "    y_min = minc[-1]\n",
    "    de_max = maxc[0]\n",
    "    de_min = minc[0]\n",
    "    data_ = (data_ - minc) / (maxc - minc)\n",
    "\n",
    "    data_last_index = data_.shape[0] - model_seq_len\n",
    "\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    for i in range(0, data_last_index - model_pre_len+1):\n",
    "        data_x = np.expand_dims(data_[i:i + model_seq_len], 0)  # [1,seq,feature_size]\n",
    "        data_y = np.expand_dims(data_[i + model_seq_len:i + model_seq_len + model_pre_len], 0)  # [1,seq,out_size]\n",
    "        data_X.append(data_x)\n",
    "        data_Y.append(data_y)\n",
    "\n",
    "    data_X=np.concatenate(data_X, axis=0)\n",
    "    data_Y = np.concatenate(data_Y, axis=0)\n",
    "\n",
    "    process_data = torch.from_numpy(data_X).type(torch.float32)\n",
    "    process_label = torch.from_numpy(data_Y).type(torch.float32)\n",
    "\n",
    "    data_feature_size = process_data.shape[-1]\n",
    "\n",
    "    dataset_train = TensorDataset(process_data, process_label)\n",
    "\n",
    "    data_dataloader = DataLoader(dataset_train, batch_size=model_batch, shuffle=False)\n",
    "    return data_dataloader,y_max,y_min, de_max,de_min\n",
    "\n",
    "def averages(matrix):  # 计算平均值\n",
    "    matrix = np.array(matrix)\n",
    "    row_count, col_count = matrix.shape\n",
    "    max_diagonal = row_count + col_count - 1\n",
    "    diagonals = np.zeros(max_diagonal)\n",
    "    counts = np.zeros(max_diagonal, dtype=int)\n",
    "    for i in range(row_count):\n",
    "        for j in range(col_count):\n",
    "            num = matrix[i, j]\n",
    "            diagonal_index = i + j\n",
    "            diagonals[diagonal_index] += num\n",
    "            counts[diagonal_index] += 1\n",
    "    averages = diagonals / counts\n",
    "    return averages\n",
    "\n",
    "def acc_loss_plot_one(train_data, type_, path):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_data, label='train_data', color='blue', linewidth=1)\n",
    "    plt.xlabel('epoch', fontsize=18)\n",
    "    plt.title(f'train_test_{type_}')\n",
    "    path_ = f'{path}'\n",
    "\n",
    "    plt.grid()\n",
    "    plt.savefig(path_)\n",
    "\n",
    "    plt.legend()\n",
    "   # plt.show()\n",
    "\n",
    "def acc_loss_plot_two(train_data, test_data, type_, path):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_data, label='train_data', color='blue', linewidth=1)\n",
    "    plt.plot(test_data, label='test_data', color='red', linewidth=1)\n",
    "    plt.xlabel('epoch', fontsize=18)\n",
    "    plt.title(f'train_test_{type_}')\n",
    "    path_ = f'{path}'\n",
    "\n",
    "    plt.grid()\n",
    "    plt.savefig(path_)\n",
    "\n",
    "    plt.legend()\n",
    "  #  plt.show()\n",
    "\n",
    "def true_test_plot(depth, true_data, predicted_data, type_, path):\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(depth, true_data, label='true_data', color='blue', linewidth=1)\n",
    "    plt.plot(depth, predicted_data, label='test_data', color='green', linewidth=1)\n",
    "    plt.ylabel(\"GRA\", fontsize=18)\n",
    "    plt.xlabel('depth', fontsize=18)\n",
    "    path_ = f'{path}'\n",
    "    plt.grid()\n",
    "    plt.savefig(path_)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d495fa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " acc = 0.965512  r2 = 0.994777  mse = 0.066961  mae = 0.227216 time =  2025-02-14 10:17:36.468483\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "data_dataloader,y_max,y_min, de_max,de_min = data_load(bh_2)\n",
    "\n",
    "model=TransAm().to(device)\n",
    "model_Path = \"./out1/model/server_model.pkl\"\n",
    "model.load_state_dict(torch.load(model_Path, map_location=torch.device(device)))\n",
    "criterion = nn.MSELoss()  # 忽略 占位符 索引为0.9\n",
    "\n",
    "\n",
    "def initiate():\n",
    "    test_acc_size = []\n",
    "    test_r2_size = []\n",
    "    test_mse_size = []\n",
    "    test_mae_size = []\n",
    "    test_loss_size = []\n",
    "\n",
    "    start = datetime.now()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_acc, test_r2, test_mse, test_mae, true_test, pre_test, test_depth = test(model, data_dataloader, y_max,\n",
    "                                                                                             y_min, de_max, de_min)\n",
    "    test_mse_size.append(test_mse)\n",
    "    test_mae_size.append(test_mae)\n",
    "    test_acc_size.append(test_acc)\n",
    "    test_r2_size.append(test_r2)\n",
    "    print(' acc =', '{:.6f}'.format(test_acc), ' r2 =', '{:.6f}'.format(test_r2),\n",
    "          ' mse =', '{:.6f}'.format(test_mse), ' mae =', '{:.6f}'.format(test_mae), 'time = ', start)\n",
    "\n",
    "    acc_mse_mae_dict = {'test_acc': test_acc_size, 'test_r2': test_r2_size,\n",
    "                             'test_mse': test_mse_size, 'test_mae': test_mae_size, }\n",
    "    acc_mse_mae = pd.DataFrame(acc_mse_mae_dict)\n",
    "\n",
    "    test_de = pd.DataFrame(test_depth, columns=['test_depth'])\n",
    "    test_t = pd.DataFrame(true_test, columns=['test_true'])\n",
    "    test_p = pd.DataFrame(pre_test, columns=['test_pre'])\n",
    "\n",
    "    csv_test = pd.concat([test_de, test_t, test_p], axis=1)\n",
    "\n",
    "    # acc_mse_mae.to_csv('./output0204/test/xj/300_300/acc_mse_mae_xj_2.csv', sep=\",\", index=True)\n",
    "    #\n",
    "    # csv_test.to_csv('./output0204/test/xj/300_300/rel_pre_test_xj_2.csv', sep=\",\", index=True)\n",
    "    #\n",
    "    # true_test_plot(csv_test['test_depth'], csv_test['test_true'], csv_test['test_pre'], 'test',\n",
    "    #                './output0204/test/xj/300_300/xj_2.png')\n",
    "    # acc_mse_mae.to_csv('./output0204/test/volve/300_300/acc_mse_mae_volve_9A.csv', sep=\",\", index=True)\n",
    "    #\n",
    "    # csv_test.to_csv('./output0204/test/volve/300_300/rel_pre_test_volve_9A.csv', sep=\",\", index=True)\n",
    "    #\n",
    "    # true_test_plot(csv_test['test_depth'], csv_test['test_true'], csv_test['test_pre'], 'test',\n",
    "    #                './output0204/test/volve/300_300/volve_9A.png')\n",
    "    # acc_mse_mae.to_csv('./output0204/test/bh/300_300/acc_mse_mae_bh_2.csv', sep=\",\", index=True)\n",
    "    #\n",
    "    # csv_test.to_csv('./output0204/test/bh/300_300/rel_pre_test_bh_2.csv', sep=\",\", index=True)\n",
    "    #\n",
    "    # true_test_plot(csv_test['test_depth'], csv_test['test_true'], csv_test['test_pre'], 'test',\n",
    "    #                './output0204/test/bh/300_300/bh_2.png')\n",
    "\n",
    "initiate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ee48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ly3610",
   "language": "python",
   "name": "py3610"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
